{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3jaOCNEiX9F",
        "outputId": "263f3ebb-1211-488a-df76-b3fba2c621ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import (\n",
        "    DataLoader, random_split\n",
        ")  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ax-avR7RiZ7i"
      },
      "outputs": [],
      "source": [
        "\n",
        "token_mapping = {\n",
        "    'start': 0,\n",
        "    'end': 1,\n",
        "    'lang_1': 'eng',\n",
        "    'lang_2': 'hin',\n",
        "    'UNK': 3,\n",
        "    'Padding_token': 4\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nH9CqYGfiyiB"
      },
      "outputs": [],
      "source": [
        "def readData(dir):\n",
        "    \"\"\"\n",
        "    Reads the data from a CSV file located at the specified directory and returns it as a Pandas DataFrame.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(dir, sep=\",\", names=['input', 'output'])\n",
        "    return data\n",
        "\n",
        "def createPairs(input_list, output_list):\n",
        "    \"\"\"\n",
        "    Takes two lists of inputs and outputs and returns a list of pairs, where each pair is a list containing an input and its corresponding output.\n",
        "    \"\"\"\n",
        "    pairs = [[input_list[i], output_list[i]] for i in range(len(input_list))]\n",
        "    return pairs\n",
        "\n",
        "def addWordsToLang(lang, words):\n",
        "    \"\"\"\n",
        "    Takes a Lang object and a list of words and adds each word to the Lang's vocabulary.\n",
        "    \"\"\"\n",
        "    for word in words:\n",
        "        lang.addAllCharactersFromWord(word)\n",
        "\n",
        "def prepareData(dir, lang_1, lang_2):\n",
        "    \"\"\"\n",
        "    Reads the data from a CSV file located at the specified directory, creates a list of pairs of inputs and outputs,\n",
        "    and creates and populates two Lang objects with the vocabulary of the inputs and outputs. Returns the Lang objects,\n",
        "    the list of pairs, and the maximum length of the inputs and outputs.\n",
        "    \"\"\"\n",
        "    data = readData(dir)\n",
        "    input_list = data['input'].to_list()\n",
        "    output_list = data['output'].to_list()\n",
        "    pairs = createPairs(input_list, output_list)\n",
        "    input_lang = dictionary(token_mapping['lang_1'])\n",
        "    output_lang = dictionary(token_mapping['lang_2'])\n",
        "    addWordsToLang(input_lang, input_list)\n",
        "    addWordsToLang(output_lang, output_list)\n",
        "    max_input_length = max([len(txt) for txt in input_list])\n",
        "    max_output_length = max([len(txt) for txt in output_list])\n",
        "    return input_lang, output_lang, pairs, max_input_length, max_output_length\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class dictionary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        chars = ['<', '>', '?', '.']\n",
        "        self.char2count = {char: 0 for char in chars}\n",
        "        self.char2index = {char: index for index, char in enumerate(chars)}\n",
        "        self.n_chars = len(chars)\n",
        "        self.index2char = {index: char for index, char in enumerate(chars)}\n",
        "\n",
        "\n",
        "    def addAllCharactersFromWord(self, word):\n",
        "        count = 0\n",
        "        while True:\n",
        "            self.addChar(word[count])\n",
        "            count += 1\n",
        "            if count == len(word):\n",
        "                break\n",
        "\n",
        "    def printValues(self):\n",
        "        print(\"char2index:\")\n",
        "        for char, index in self.char2index.items():\n",
        "           print(f\"  {char}: {index}\")\n",
        "    \n",
        "        print(\"char2count:\")\n",
        "        for char, count in self.char2count.items():\n",
        "          print(f\"  {char}: {count}\")\n",
        "    \n",
        "        print(\"index2char:\")\n",
        "        for index, char in self.index2char.items():\n",
        "          print(f\"  {index}: {char}\")\n",
        "   \n",
        "    def addChar(self, char):\n",
        "       if char not in self.char2index:\n",
        "          self.index2char[self.n_chars] = char\n",
        "          self.char2index[char] = self.n_chars\n",
        "          self.char2count[char] = 0\n",
        "          self.n_chars += 1\n",
        "       self.char2count[char] += 1\n"
      ],
      "metadata": {
        "id": "8mLhsHHocKqx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SvgZ_4Rti3LX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class encodeText(nn.Module):\n",
        "    def __init__(self, input_size, configuration):\n",
        "\n",
        "      super(encodeText, self).__init__()\n",
        "      self.hidden_size = configuration['hidden_size']\n",
        "      self.cell_type = configuration[\"cell_type\"]\n",
        "      self.batch_size = configuration['batch_size']\n",
        "      self.dropout = nn.Dropout(configuration['drop_out']) \n",
        "      self.embedding_size = configuration['embedding_size']\n",
        "      self.bidirectional = configuration['bi_directional']\n",
        "      self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
        "      \n",
        "\n",
        "    # Initialize the recurrent unit layer.\n",
        "      if self.cell_type == 'LSTM':\n",
        "         self.cell_layer = nn.LSTM(\n",
        "             self.embedding_size,\n",
        "             self.hidden_size,\n",
        "             num_layers=configuration[\"num_layers_encoder\"],\n",
        "             dropout=configuration['drop_out'],\n",
        "             bidirectional=configuration['bi_directional']\n",
        "        )\n",
        "      elif self.cell_type == 'GRU':\n",
        "         self.cell_layer = nn.GRU(\n",
        "             self.embedding_size,\n",
        "             self.hidden_size,\n",
        "             num_layers=configuration[\"num_layers_encoder\"],\n",
        "             dropout=configuration['drop_out'],\n",
        "             bidirectional=configuration['bi_directional']\n",
        "        )\n",
        "      else: \n",
        "         self.cell_layer = nn.RNN(\n",
        "             self.embedding_size,\n",
        "             self.hidden_size,\n",
        "             num_layers=configuration[\"num_layers_encoder\"],\n",
        "             dropout=configuration['drop_out'],\n",
        "             bidirectional=configuration['bi_directional']\n",
        "        )\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "      \n",
        "        weight = self.embedding.weight.to(input.device)\n",
        "        embedded = F.embedding(input, weight)\n",
        "        embedded = self.dropout(embedded.view(1, self.batch_size, -1))\n",
        "        output, hidden = self.cell_layer(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        " \n",
        "\n",
        "    def initializeHiddenState(self, num_layers):\n",
        "        enable_gpu=torch.cuda.is_available()\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        hidden_size = self.hidden_size // num_directions\n",
        "        res = torch.zeros(num_layers * num_directions, self.batch_size, hidden_size)\n",
        "        if enable_gpu:\n",
        "           res = res.cuda()\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Sv0K-T9i9Mo"
      },
      "outputs": [],
      "source": [
        "class decodeText(nn.Module):\n",
        "    def __init__(self, configuration, output_size):\n",
        "\n",
        "        super(decodeText, self).__init__()\n",
        "        # Save the configuration parameters.\n",
        "        self.cell_type = configuration[\"cell_type\"]\n",
        "        self.hidden_size = configuration['hidden_size']\n",
        "        self.batch_size = configuration['batch_size']\n",
        "        self.num_layers = configuration['num_layers_decoder']\n",
        "        self.embedding_size = configuration['embedding_size']\n",
        "        self.bidirectional = configuration['bi_directional']\n",
        "        self.embedding = nn.Embedding(output_size, self.embedding_size)\n",
        "        self.dropout = nn.Dropout(configuration['drop_out'])\n",
        "\n",
        "        # Initialize the recurrent unit layer.\n",
        "        if self.cell_type == 'RNN':\n",
        "            self.cell_layer = nn.RNN(\n",
        "                self.embedding_size,\n",
        "                self.hidden_size,\n",
        "                num_layers=self.num_layers,\n",
        "                dropout=configuration['drop_out'],\n",
        "                bidirectional=self.bidirectional\n",
        "            )\n",
        "        elif self.cell_type == 'GRU':\n",
        "            self.cell_layer = nn.GRU(\n",
        "                self.embedding_size,\n",
        "                self.hidden_size,\n",
        "                num_layers=self.num_layers,\n",
        "                dropout=configuration['drop_out'],\n",
        "                bidirectional=self.bidirectional\n",
        "            )\n",
        "        elif self.cell_type == 'LSTM':\n",
        "            self.cell_layer = nn.LSTM(\n",
        "                self.embedding_size,\n",
        "                self.hidden_size,\n",
        "                num_layers=self.num_layers,\n",
        "                dropout=configuration['drop_out'],\n",
        "                bidirectional=self.bidirectional\n",
        "            )\n",
        "            \n",
        "        self.out = nn.Linear(self.hidden_size, output_size)\n",
        "        if self.bidirectional:\n",
        "            self.out = nn.Linear(self.hidden_size * 2, output_size)\n",
        "\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.dropout(self.embedding(input).view(1, self.batch_size, -1))\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.cell_layer(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initializeHiddenState(self):\n",
        "\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        res = torch.zeros(self.num_layers * num_directions, self.batch_size, self.hidden_size)\n",
        "        if enable_gpu:\n",
        "            return res.cuda()\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5BGC9Zfci-kb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def indexesFromWord(lang, word):\n",
        "    \"\"\"\n",
        "    Converts a word to a list of indexes.\n",
        "\n",
        "    Args:\n",
        "        lang: The language model.\n",
        "        word: The word to convert.\n",
        "\n",
        "    Returns:\n",
        "        A list of indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    return [lang.char2index[char] for char in word]\n",
        "\n",
        "\n",
        "def variableFromSentence(lang, sentence, max_length):\n",
        "    \"\"\"\n",
        "    Converts a sentence to a variable.\n",
        "\n",
        "    Args:\n",
        "        lang: The language model.\n",
        "        sentence: The sentence to convert.\n",
        "        max_length: The maximum length of the sentence.\n",
        "\n",
        "    Returns:\n",
        "        A variable.\n",
        "    \"\"\"\n",
        "\n",
        "    indexes = indexesFromWord(lang, sentence)\n",
        "    indexes.append(token_mapping['end'])\n",
        "    indexes.extend([token_mapping['Padding_token']] * (max_length - len(indexes)))\n",
        "    if(enable_gpu):\n",
        "      return torch.LongTensor(indexes).cuda()\n",
        "      \n",
        "    return torch.LongTensor(indexes)\n",
        "\n",
        "\n",
        "def variablesFromPairs(input_lang, output_lang, pairs, max_length):\n",
        "    \"\"\"\n",
        "    Converts a list of pairs to a list of variables.\n",
        "\n",
        "    Args:\n",
        "        input_lang: The input language model.\n",
        "        output_lang: The output language model.\n",
        "        pairs: The list of pairs to convert.\n",
        "        max_length: The maximum length of the sentences.\n",
        "\n",
        "    Returns:\n",
        "        A list of variables.\n",
        "    \"\"\"\n",
        "\n",
        "    res = []\n",
        "    for pair in pairs:\n",
        "        input_variable = variableFromSentence(input_lang, pair[0], max_length)\n",
        "        output_variable = variableFromSentence(output_lang, pair[1], max_length)\n",
        "        res.append((input_variable, output_variable))\n",
        "    return res\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GJl_ew2PjHWh"
      },
      "outputs": [],
      "source": [
        "def modelEvaluation(encoder, decoder, loader, configuration, criterion, max_length, output_lang):\n",
        "    \"\"\"\n",
        "    modelEvaluations the performance of the encoder-decoder model on the given data.\n",
        "\n",
        "    Args:\n",
        "        encoder: The encoder model.\n",
        "        decoder: The decoder model.\n",
        "        loader: The data loader.\n",
        "        configuration: The configuration parameters.\n",
        "        criterion: The loss function.\n",
        "        max_length: The maximum length of a sequence.\n",
        "        output_lang: The output language.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy and loss of the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    batch_size = configuration['batch_size']\n",
        "    loss ,total,correct= 0,0,0\n",
        "    enable_gpu = torch.cuda.is_available()\n",
        "    \n",
        "\n",
        "    for batch_input, batch_output in loader:\n",
        "        batch_loss = 0\n",
        "        numLayersEncoder = configuration['num_layers_encoder']\n",
        "        encoder_hidden = encoder.initializeHiddenState(numLayersEncoder)\n",
        "        if configuration[\"cell_type\"] == \"LSTM\":\n",
        "            encoder_cell_state = encoder.initializeHiddenState(numLayersEncoder)\n",
        "            encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
        "        # if \"cell_type\" in configuration and configuration[\"cell_type\"] == \"LSTM\":\n",
        "        #     encoder_hidden = (encoder_hidden, encoder.initializeHiddenState(configuration['num_layers_encoder'])[1])\n",
        "\n",
        "\n",
        "        input_variable = batch_input.transpose(0, 1)\n",
        "        output_variable = batch_output.transpose(0, 1)\n",
        "\n",
        "        input_length = input_variable.size(0)\n",
        "        target_length = output_variable.size(0)\n",
        "\n",
        "        output = torch.LongTensor(target_length, batch_size)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
        "        if enable_gpu:\n",
        "            encoder_outputs = encoder_outputs.cuda()\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_variable[i], encoder_hidden)\n",
        "            encoder_outputs[i] = encoder_output\n",
        "\n",
        "        decoder_input = torch.LongTensor([token_mapping['start']] * batch_size)\n",
        "        if enable_gpu:\n",
        "            decoder_input = decoder_input.cuda()\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        for j in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            temp1 =criterion(decoder_output, output_variable[j].squeeze())\n",
        "            batch_loss = batch_loss + temp1\n",
        "            _ , topi = decoder_output.data.topk(1)\n",
        "            decoder_input = torch.cat((topi.squeeze(),))\n",
        "\n",
        "            output[j] = topi.squeeze()\n",
        "\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        for k in range(output.size(0)):\n",
        "            ignore = [token_mapping['start'], token_mapping['end'], token_mapping['Padding_token']]\n",
        "            sent = [output_lang.index2char[letter.item()] for letter in output[k] if letter.item() not in ignore]\n",
        "            y = [output_lang.index2char[letter.item()] for letter in batch_output[k] if letter.item() not in ignore]\n",
        "            # print(sent)\n",
        "            # print(\"prediciton\")\n",
        "            # print(y)\n",
        "            if sent != y:\n",
        "                correct =correct\n",
        "            else:\n",
        "              correct = correct+1\n",
        "\n",
        "            total=total+1\n",
        "            # correct = correct + 1 if sent == 'y' else correct\n",
        "            # total = total + 1\n",
        "\n",
        "\n",
        "        accuracy = (correct/total) * 100\n",
        "        temp1= batch_loss.item() / target_length\n",
        "        loss = loss + temp1\n",
        "    return accuracy, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ShVfNyYTrRaV"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, output_tensor, encoder, decoder,train_loader,val_loader, encoder_optimizer, decoder_optimizer, criterion, configuration, max_length,batch_size):\n",
        "\n",
        "    #batch_size = configuration['batch_size']\n",
        "    enable_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    numLayersEncoder = configuration['num_layers_encoder']\n",
        "    encoder_hidden = encoder.initializeHiddenState(numLayersEncoder)\n",
        "    if configuration[\"cell_type\"] == \"LSTM\":\n",
        "        encoder_cell_state = encoder.initializeHiddenState(numLayersEncoder)\n",
        "        encoder_hidden = (encoder_hidden, encoder_cell_state)\n",
        "\n",
        "    input_tensor = input_tensor.transpose(0, 1)\n",
        "    output_tensor = output_tensor.transpose(0, 1)\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, batch_size, encoder.hidden_size)\n",
        "    if enable_gpu:\n",
        "        encoder_outputs = encoder_outputs.cuda()\n",
        "\n",
        "    loss = 0\n",
        "    input_length = input_tensor.size(0)\n",
        "    output_length = output_tensor.size(0)\n",
        "\n",
        "    encoder_hidden = encodeInputSequence(encoder, input_tensor, encoder_hidden, input_length)\n",
        "\n",
        "    decoder_input = torch.LongTensor([token_mapping['start']] * batch_size)\n",
        "    if enable_gpu:\n",
        "        decoder_input = decoder_input.cuda()\n",
        "    teacher_forcing_ratio = configuration['teacher_forcing_ratio']\n",
        "    decoder_hidden = encoder_hidden\n",
        "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        loss = decodeWithTeacherForcing(decoder, decoder_input, decoder_hidden, output_tensor, criterion, output_length, loss)\n",
        "    else:\n",
        "        loss = decodeWithoutTeacherForcing(decoder, decoder_input, decoder_hidden, output_tensor, criterion, output_length, loss, enable_gpu)\n",
        "\n",
        "    loss /= output_length\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def encodeInputSequence(encoder, input_tensor, encoder_hidden, input_length):\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "    return encoder_hidden\n",
        "\n",
        "def decodeWithTeacherForcing(decoder, decoder_input, decoder_hidden, output_tensor, criterion, output_length, loss):\n",
        "    for i in range(output_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        decoder_input = output_tensor[i]\n",
        "        loss = loss + criterion(decoder_output, output_tensor[i])\n",
        "    return loss\n",
        "\n",
        "def decodeWithoutTeacherForcing(decoder, decoder_input, decoder_hidden, output_tensor, criterion, output_length, loss, enable_gpu):\n",
        "    for i in range(output_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        _,topi= decoder_output.data.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "        if enable_gpu:\n",
        "            decoder_input = decoder_input.cuda()\n",
        "        loss =loss + criterion(decoder_output, output_tensor[i])\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hjVZxfuLiTyj"
      },
      "outputs": [],
      "source": [
        "def trainAndEvaluate(encoder, decoder, train_loader, val_loader, configuration, max_len, max_len_all,input_lang,output_lang):\n",
        "    #print(\"hello5\")\n",
        "    z=configuration['learning_rate']\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=z)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=z)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(configuration['epochs']):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, configuration['epochs']))\n",
        "        train_loss = 0\n",
        "   \n",
        "        batch_no = 1\n",
        "        for batch_input, batch_output in train_loader:\n",
        "            loss = None\n",
        "            if not configuration['attention']:\n",
        "                # print(\"hello6\")\n",
        "                loss = train(batch_input, batch_output, encoder, decoder,train_loader,val_loader,encoder_optimizer, decoder_optimizer, criterion, configuration, max_len_all,configuration['batch_size'])\n",
        "                # print(\"hello7\")\n",
        "\n",
        "        train_loss += loss\n",
        "        batch_no += 1\n",
        "\n",
        "        print('Train loss: {}'.format(train_loss / len(train_loader)))\n",
        "\n",
        "        validation_accuracy, validation_loss = modelEvaluation(encoder, decoder, val_loader, configuration, criterion, max_len, output_lang)\n",
        "#        print(\"hello8\")\n",
        "        print('Validation loss: {}'.format(validation_loss / len(val_loader)))\n",
        "        print('Validation accuracy: {}'.format(validation_accuracy))\n",
        "\n",
        "        \n",
        "\n",
        "configuration = {\n",
        "            \"hidden_size\" : 256,\n",
        "            \"input_lang\" : 'eng',\n",
        "            \"output_lang\" : 'hin',\n",
        "            \"cell_type\"   : 'RNN',\n",
        "            \"num_layers_encoder\" : 1 ,\n",
        "            \"num_layers_decoder\" : 1,\n",
        "            \"drop_out\"    : 0, \n",
        "            \"embedding_size\" : 128,\n",
        "            \"bi_directional\" : False,\n",
        "            \"batch_size\" : 128,\n",
        "            \"attention\" : False ,\n",
        "            \"learning_rate\" : 0.001,\n",
        "            \"epochs\":10,\n",
        "            \"teacher_forcing_ratio\": 0.5\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# sweep_config ={\n",
        "#     'method':'bayes'\n",
        "# }\n",
        "\n",
        "# metric = {\n",
        "#     'name' : 'validation_accuracy',\n",
        "#     'goal' : 'maximize'\n",
        "# }\n",
        "# sweep_config['metric'] = metric\n",
        "\n",
        "# parameters_dict={\n",
        "#      'ep':{\n",
        "#         'values' :[5,8,10]\n",
        "#     },\n",
        "#     'hidden_size':{\n",
        "#         'values' : [64,128,256,512]\n",
        "#     },\n",
        "#     'learning_rate':{\n",
        "#         'values' : [1e-2,1e-3,1e-1]\n",
        "#     },\n",
        "#      'bidirectional':{\n",
        "#         'values' : [True,False]\n",
        "#     },\n",
        "#     'cell_type':{\n",
        "#         'values' : ['LSTM','GRU','RNN']\n",
        "#     },\n",
        "#     'encoder_layers':{\n",
        "#         'values' : [1,2,3]\n",
        "#     },\n",
        "#     'decoder_layers':{\n",
        "#         'values' : [1,2,3]\n",
        "#     },\n",
        "#     'drop_out':{\n",
        "#         'values' : [0.0,0.2,0.3]\n",
        "#     },\n",
        "#     'embedding_size':{\n",
        "#         'values' : [64,128,256,512]\n",
        "#     },\n",
        "#     'batch_size':{\n",
        "#         'values' : [32,64,128]\n",
        "#     }\n",
        "   \n",
        "# }\n",
        "# sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, project = 'dl_assignement_3')"
      ],
      "metadata": {
        "id": "xjdHneCcuXaa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WbrvV0Crc8z2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "def prepare_data(configuration,file_path, lang_1, lang_2):\n",
        "    input_lang, output_lang, pairs, max_input_length, max_target_length = prepareData(file_path, lang_1, lang_2)\n",
        "    return  input_lang, output_lang, pairs, max_input_length, max_target_length\n",
        "    \n",
        "\n",
        "def variables_from_pairs(input_lang, output_lang, pairs, max_len):\n",
        "    return [input_lang.variable_from_pair(pair, max_len) for pair in pairs]\n",
        "\n",
        "def train_model(encoder, decoder, train_loader, val_loader, configuration, max_len, max_len_all,input_lang,output_lang,enable_gpu):\n",
        "    d = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if(enable_gpu):\n",
        "      encoder.to(device)\n",
        "      decoder.to(device)\n",
        "\n",
        "    trainAndEvaluate(encoder, decoder, train_loader, val_loader, configuration, max_len, max_len_all,input_lang, output_lang)\n",
        "\n",
        "\n",
        "dir = '/content/aksharantar'\n",
        "\n",
        "# def train_language_model(dir, lang_1, lang_2, configuration):\n",
        "#     train_path = os.path.join(dir, lang_2, f\"{lang_2}_train.csv\")\n",
        "#     test_path = os.path.join(dir, lang_2, f\"{lang_2}_test.csv\")\n",
        "#     validation_path = os.path.join(dir, lang_2, f\"{lang_2}_valid.csv\")\n",
        "    \n",
        "#     data_paths = [train_path, test_path, validation_path]\n",
        "#     max_lengths = []\n",
        "#     prepared_data = []\n",
        "\n",
        "#     for path in data_paths:\n",
        "#         input_lang, output_lang, pairs, max_input_length, max_target_length = prepare_data(path, lang_1, lang_2)\n",
        "#         prepared_data.append((input_lang, output_lang, pairs))\n",
        "#         max_lengths.append((max_input_length, max_target_length))\n",
        "\n",
        "#     (input_lang, output_lang, pairs), (test_input_lang, test_output_lang, test_pairs), (val_input_lang, val_output_lang, val_pairs) = prepared_data\n",
        "#     (max_input_length, max_target_length), (max_input_length_test, max_target_length_test), (max_input_length_val, max_target_length_val) = max_lengths\n",
        "\n",
        "#     return (input_lang, output_lang, pairs, max_input_length, max_target_length), \\\n",
        "#            (test_input_lang, test_output_lang, test_pairs, max_input_length_test, max_target_length_test), \\\n",
        "#            (val_input_lang, val_output_lang, val_pairs, max_input_length_val, max_target_length_val)\n",
        "\n",
        "\n",
        "\n",
        "def train_language_model(dir, lang_1, lang_2, configuration,batch_size,enable_gpu):\n",
        "    train_path = os.path.join(dir, lang_2, lang_2 + '_train.csv')\n",
        "    test_path = os.path.join(dir, lang_2, lang_2 + '_test.csv')\n",
        "    validation_path = os.path.join(dir, lang_2, lang_2 + '_valid.csv')\n",
        "    input_lang, output_lang, pairs, max_input_length, max_target_length = prepare_data(configuration,train_path, lang_1, lang_2,)\n",
        "    test_input_lang, test_output_lang, test_pairs, max_input_length_test, max_target_length_test = prepare_data(configuration,test_path, lang_1, lang_2)\n",
        "    val_input_lang, val_output_lang, val_pairs, max_input_length_val, max_target_length_val = prepare_data(configuration,validation_path, lang_1, lang_2)\n",
        "    \n",
        "    print(\"checkpoint-1\")\n",
        "    print(random.choice(pairs))\n",
        "\n",
        "    \n",
        "    max_list = [max_input_length, max_target_length, max_input_length_val, max_target_length_val, max_input_length_test, max_target_length_test]\n",
        "    max_len_all = sorted(max_list)[-1]\n",
        "    max_len = max(max_input_length, max_target_length)\n",
        "    max_len +=2\n",
        "    print(\"checkpoint-2\")\n",
        "\n",
        "\n",
        "   \n",
        "    pairs = variablesFromPairs(input_lang, output_lang, pairs, max_len)\n",
        "    val_pairs = variablesFromPairs(input_lang, output_lang, val_pairs, max_len_all)\n",
        "    print(\"checkpoint-3\")\n",
        "        \n",
        "\n",
        "    text_encoder = encodeText(input_lang.n_chars, configuration)\n",
        "    text_decoder = decodeText(configuration, output_lang.n_chars)\n",
        "    print(\"checkpoint-4\")\n",
        "    trainDataloader = torch.utils.data.DataLoader(pairs, batch_size=batch_size, shuffle=True)\n",
        "    valDataloader = torch.utils.data.DataLoader(val_pairs, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    \n",
        "\n",
        "    if not configuration['attention']:\n",
        "        train_model(text_encoder,text_decoder, trainDataloader, valDataloader, configuration, max_len, max_len_all,input_lang,output_lang,enable_gpu)\n",
        "        print(\"Code is successfully Executed...\")\n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJvK0-fci7KW",
        "outputId": "27ff4e6e-480e-4485-ebf4-bba98b090e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint-1\n",
            "['methoxyphenoside', 'मेथोक्सीफेनोसाइड']\n",
            "checkpoint-2\n",
            "checkpoint-3\n",
            "checkpoint-4\n",
            "Epoch 1/10\n",
            "Train loss: 0.002463042140007019\n",
            "Validation loss: 1.104401503617947\n",
            "Validation accuracy: 0.0\n",
            "Epoch 2/10\n",
            "Train loss: 0.0024008122086524963\n",
            "Validation loss: 1.1296484653766337\n",
            "Validation accuracy: 0.0\n",
            "Epoch 3/10\n",
            "Train loss: 0.0024573513865470886\n",
            "Validation loss: 1.1061523052362294\n",
            "Validation accuracy: 0.0\n",
            "Epoch 4/10\n",
            "Train loss: 0.0023534533381462095\n",
            "Validation loss: 1.12351799928225\n",
            "Validation accuracy: 0.0\n",
            "Epoch 5/10\n",
            "Train loss: 0.003140915036201477\n",
            "Validation loss: 1.1001181487853708\n",
            "Validation accuracy: 0.0\n",
            "Epoch 6/10\n",
            "Train loss: 0.003334808349609375\n",
            "Validation loss: 1.1374471531464503\n",
            "Validation accuracy: 0.0\n",
            "Epoch 7/10\n",
            "Train loss: 0.002315763235092163\n",
            "Validation loss: 1.1129898818639608\n",
            "Validation accuracy: 0.0\n",
            "Epoch 8/10\n",
            "Train loss: 0.002186257988214493\n",
            "Validation loss: 1.9810772859133203\n",
            "Validation accuracy: 0.0\n",
            "Epoch 9/10\n",
            "Train loss: 0.0022124762833118437\n",
            "Validation loss: 1.1168325612178216\n",
            "Validation accuracy: 0.0\n",
            "Epoch 10/10\n",
            "Train loss: 0.0022408953309059143\n",
            "Validation loss: 1.1251973853661463\n",
            "Validation accuracy: 0.0\n",
            "Code is successfully Executed...\n"
          ]
        }
      ],
      "source": [
        "enable_gpu= torch.cuda.is_available()\n",
        "train_language_model(dir, token_mapping['lang_1'], token_mapping['lang_2'], configuration,configuration['batch_size'],enable_gpu)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
